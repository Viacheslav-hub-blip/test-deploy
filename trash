тестирование агентов, моменты:
- необходимо делать несколько перезапусков каждого кейса, так как модели дают разные результаты 
- если агент сделал какое то 'реальное' дейтсвие: запись в базу, генерация и выполнение кода, необходимо проверять действительно ли он выполнил действие 
или просто сказал, что дейсвтвие выполнено 
- набор: 20-50 тестов 
- в наборе должны быть задачи не которые агент точно умеет решать, а проблемные запросы
- '''Тестируйте как случаи, когда поведение должно проявляться, так и случаи, когда оно не должно проявляться. Односторонняя оценка приводит к односторонней оптимизации. Например, если вы проверяете только то, выполняет ли агент поиск, когда это необходимо, в итоге может получиться агент, который ищет практически все подряд. Старайтесь избегать несбалансированных оценок'''
(добавить запросы на которые агент не должен ответить (выполнить действие))
- Важнейшая метрика – успешность выполнения задачи (Task Success / Task Completion): соответствует ли итоговый ответ или состояние системы заданной цели. Это можно измерять бинарно (успел/нет) или по шкале (например, процент выполненных шагов)

- Во-вторых, критерии оценки отклоняются от бизнес-целей. Команды ориентируются на такие показатели, как сокращение количества вызовов API или логичность рассуждений, которые не отражают того, что действительно важно: была ли решена проблема клиента?
- спам тестирование ( отправка заранее мусорных вопросов), которые агент не сможет понять. 
- я думаю добавить специфичные проверки: например передача в аргументах инструмента строк которые содержат {"key"}, так как такая строка сломает prompt запрос в langchain



- отдельно тестировать каждый инстурмент (на разные входные параметры и возможность отказа работы) 
- отдельно тестировать правильность выбора инструмента на отдельных запросах (оцениваем не ответ, а правильность выбора инструмента)
- отдельно тестируем правильность конечного ответа (соответвует ли полученный результат эталонному)
- отдельно тестируем правильность цепочки вызовов - ? Непонятно нужно ли, только если есть какая то строгая последовательность 




Многократные прогоны: каждый кейс запускать несколько раз (рекомендуется 5–10), агрегировать результат (success_rate, медиана, дисперсия). Модель детерминирована не всегда — это обязательно учитывать.
Side-effect verification: если агент «сказал, что выполнил действие» — обязательно проверить факт (запись в БД, созданный тикет, HTTP-запрос, выполнение кода в sandbox). Текст в ответе — не доказательство.
Набор тестов 20–50: разумный стартовый объём. Включать mix: «умеет точно», «проблемные/пограничные», и «адверсариальные/спам».
Negative tests: включать кейсы, в которых агент не должен выполнять действие (иначе он научится «всегда вызывать» инструменты).
Спам / мусорные запросы: отдельная категория — как агент отказывается или просит уточнение.
Тестировать инструменты отдельно: на входные данные, ошибки, таймауты, неправильные аргументы.
Тестировать выбор инструмента отдельно: не всегда оцениваем финальный ответ — иногда важно, чтобы выбран был правильный инструмент и корректные аргументы.
Тестировать финальный ответ отдельно: соответствует ли результат эталону (Task Success).
Тестировать цепочки вызовов (trajectory) — только если в системе есть строгие требования к порядку; 



